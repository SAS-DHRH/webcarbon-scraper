{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website Carbon scraper\n",
    "\n",
    "This script takes a list of URL's in the urls.txt file, cleans the URL's, and scrapes the websitecarbon.com page.\n",
    "It then extracts the grams of carbon value from the HTML, and saves the results to the carbondata.csv file.\n",
    "\n",
    "I started by attempting to use MS Excel's cell formulas =WEBSERVICE() and =FILTERXML() functions, but these only work on windows. I then decided to try learning Julia, but decided it would be quicker to just use python and command line. The first version of this python notebook used wget to scrape the URL's, but in the end I needed to use requests.sessions and post the URL to the Websitecarbon.com webform to reliably retrieve the carbon data.\n",
    "\n",
    "To rerun this script, check the list of URL's in the urls.txt file are up to date, clear out any cached HTML files in the ./html directory, and run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the URL list a little bit\n",
    "urls = open('urls.txt').readlines()\n",
    "urls = [u.rstrip('\\n').strip().rstrip('/').replace('https://', '').replace('http://', '').replace('www.', '') for u in urls]\n",
    "urls = [u for u in urls if u]  # remove empties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWebsitecarbon(url):\n",
    "    '''\n",
    "    Function takes a URL and scrapes the grams of carbon from the websitecarbon.com site.\n",
    "    It saves the response HTML file and extracts the data from the HTML header\n",
    "    Returns a quadruple (url, source, metric type, grams of CO2)\n",
    "    '''\n",
    "    source = 'websitecarbon.com'\n",
    "    carbonurl = 'https://www.websitecarbon.com'\n",
    "    cleanurl = url.replace('.', '-').replace('/', '-')\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    payload = {'wgd-cc-url':url,\n",
    "               'wgd-cc-retest': 'true'}\n",
    "    \n",
    "    scrapefile = os.path.join(\"./html/\", source, cleanurl + \".html\")\n",
    "    directory = os.path.dirname(scrapefile)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Make the request and save the response\n",
    "    if not os.path.exists(scrapefile) or os.path.getsize(scrapefile) == 0:\n",
    "        print('scraping ' + url)\n",
    "        session = requests.Session()\n",
    "        sess = session.post(carbonurl, headers=headers, data=payload)\n",
    "        with open(scrapefile, \"w\") as f:\n",
    "            f.write(sess.text)\n",
    "            f.close()\n",
    "    else:\n",
    "        print('using cached html for ' + url)\n",
    "        \n",
    "    # Load the cached response file and search for the data\n",
    "    grams = !grep -E 'grams\": (.*),$' {scrapefile} | cut -d: -f2 | cut -d, -f1\n",
    "    if len(grams) > 0:\n",
    "        grams = grams[0].strip()\n",
    "    \n",
    "    return (url, source, 'grams of CO2', grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Emissions.com is another website carbon calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWebsiteemissions(url):\n",
    "    '''\n",
    "    Function takes a URL and scrapes the grams of carbon from the https://websiteemissions.com site.\n",
    "    Returns a quadruple (url, source, metric type, grams of CO2)\n",
    "    '''\n",
    "    source = 'websiteemissions.com'\n",
    "    carbonurl = 'https://websiteemissions.com/'\n",
    "    carbonajaxurl = 'https://websiteemissions.com/wp-admin/admin-ajax.php'\n",
    "    cleanurl = url.replace('.', '-').replace('/', '-')\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    # The payload needs the nonce added later\n",
    "    payload = {'action': 'carbon_calculate',\n",
    "               'weblink': 'https://' + url}  # this has to start with https://\n",
    "\n",
    "    scrapefile = os.path.join(\"./html/\", source, cleanurl + \".html\")\n",
    "    directory = os.path.dirname(scrapefile)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Make the request and save the response - so we can get the wordpress nonce value\n",
    "    if not os.path.exists(scrapefile) or os.path.getsize(scrapefile) == 0:\n",
    "        session = requests.Session()\n",
    "        sess = session.get(carbonurl)\n",
    "\n",
    "        # This is the line of javascript we need to get the wordpress nonce value from\n",
    "        noncepattern = 'var carbon_calc_ajax.*?nonce\":\"(.*?)\"'\n",
    "        result = re.findall(noncepattern, sess.text)\n",
    "\n",
    "        # Once we have the nonce, we can POST the webform to retrieve the response data\n",
    "        if result:\n",
    "            print('scraping ' + url)\n",
    "            payload['carbonNonce'] = result[0]  # this is a required wordpress nonce value\n",
    "            sess = session.post(carbonajaxurl, headers=headers, data=payload)\n",
    "            with open(scrapefile, \"w\") as f:\n",
    "                f.write(sess.text)\n",
    "                f.close()\n",
    "    else:\n",
    "        print('using cached html for ' + url)\n",
    "\n",
    "    # read file and get json data\n",
    "    data = None\n",
    "    with open(scrapefile, 'r') as f:\n",
    "        data = f.read()\n",
    "        data = json.loads(data)\n",
    "    \n",
    "    return (url, source, 'grams of CO2', data['co2']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the URL's into a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbondata = [scrapeWebsitecarbon(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webemissions = [scrapeWebsiteemissions(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lists\n",
    "carbondata.extend(webemissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "with open('carbondata.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['url', 'source', 'type', 'value'])\n",
    "    writer.writerows(carbondata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can multiply these values by your website pageviews analytics to calculate your website's carbon footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "carbondata = pd.read_csv('carbondata.csv')\n",
    "carbondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
